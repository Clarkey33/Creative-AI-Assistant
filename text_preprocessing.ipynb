{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "#os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # Force PyTorch mode\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 text files.\n"
     ]
    }
   ],
   "source": [
    "#load json file\n",
    "\n",
    "with open(\"training_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(data)} text files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Text\n",
    "\n",
    "prepare text for use in pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' \n",
    "    Remove unwanted formatting, line breaks, and unnecessary symbols.\n",
    "    Normalize text (convert to lowercase, remove extra spaces).\n",
    "\n",
    "    Args:\n",
    "    text: json file with text for training \n",
    "\n",
    "    returns: processed text\n",
    "    '''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    text = re.sub(r\"[^\\w\\s,.!?]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s,.!?;:()\\\"'-]\", \"\", text)  # Keep standard characters\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to all text data\n",
    "for item in data:\n",
    "    item[\"cleaned_content\"] = clean_text(item[\"content\"])\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His value unquestioned\n",
      "His worth immeasurable\n",
      "Scaloni knew this was the piece to preserve if they were to make the finals\n",
      "Di Maria was scarcely used in the knockout stages\n",
      "And for 63 min of the finals \n",
      "He left Everything on the field \n",
      "A man for the Big Occasion\n",
      "in '05 U20 World Cup\n",
      "'08 Olympic Gold\n",
      "'21 Copa America Champions\n",
      "'22 World Cup Champions\n",
      "\n",
      "his value unquestioned his worth immeasurable scaloni knew this was the piece to preserve if they were to make the finals di maria was scarcely used in the knockout stages and for 63 min of the finals he left everything on the field a man for the big occasion in 05 u20 world cup 08 olympic gold 21 copa america champions 22 world cup champions\n"
     ]
    }
   ],
   "source": [
    "#preview\n",
    "slices = data[1:2]\n",
    "\n",
    "for val in slices:\n",
    "    print(val['content'])\n",
    "    print(val['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clean text for fine tune training\n",
    "formatted_data = []\n",
    "for item in data:\n",
    "    formatted_data.append({\n",
    "        \"prompt\": f\"What is the story of {item['file_name'].replace('_', ' ').replace('.txt', '')}?\",\n",
    "        \"response\": item[\"cleaned_content\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the story of argentina unsung hero?\n",
      "his value unquestioned his worth immeasurable scaloni knew this was the piece to preserve if they were to make the finals di maria was scarcely used in the knockout stages and for 63 min of the finals he left everything on the field a man for the big occasion in 05 u20 world cup 08 olympic gold 21 copa america champions 22 world cup champions\n"
     ]
    }
   ],
   "source": [
    "#preview\n",
    "slices = formatted_data[1:2]\n",
    "\n",
    "for val in slices:\n",
    "    print(val['prompt'])\n",
    "    print(val['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSONL file\n",
    "with open(\"ft_training_data.jsonl\", \"w\") as f:\n",
    "    for item in formatted_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine Tuning: Peft LoRA technique\n",
    "\n",
    "Reduces GPU memory usage\n",
    "Fine-tunes faster\n",
    "Only trains a small subset of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign llama 3.2 to variable name\n",
    "model_llama_32 = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer_llama_32 = AutoTokenizer.from_pretrained(model_llama_32)\n",
    "\n",
    "# Load Model on CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Enable 4-bit quantization to reduce memory usage\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_llama_32, \n",
    "                                                       device_map=\"auto\",\n",
    "                                                       low_cpu_mem_usage=True,\n",
    "                                                       torch_dtype=torch.bfloat16,\n",
    "                                                       quantization_config=quantization_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16, #Lora rank: lower = faster, higher = more fine tuning effect\n",
    "    lora_alpha=16, #scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], ## Apply LoRA to query and value projection layers\n",
    "    lora_dropout=0.05, # Dropout for better generalization\n",
    "    bias=\"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "    #modules_to_save=[\"classifier\"],\n",
    "#)\n",
    "\n",
    "model_lora = get_peft_model(quantized_model,lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fed91a77c74be1a2a3b3673c940a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load dataset for fine tuning\n",
    "ft_dataset = load_dataset(\"json\", data_files=\"ft_training_data.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What is the story of achievment unlocked?', 'response': 'he was herald as the successor to maradona he matched his world cup record assists with 8 he outpaced his goal talley with 13 16 the record by klose to which he couldnt be any closer but the child of promise waited 16 years to return glory to argentina 36 years later'}\n"
     ]
    }
   ],
   "source": [
    "#check for correct data structure\n",
    "print(ft_dataset[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fbe32b7079425584691f2ce7c7fe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the pad_token to the EOS token o\n",
    "tokenizer_llama_32.pad_token = tokenizer_llama_32.eos_token  \n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(text):\n",
    "    #combined_text = text[\"prompt\"] + \"\\n\" + text[\"response\"] \n",
    "    combined_text = [p + \"\\n\" + r for p, r in zip(text[\"prompt\"], text[\"response\"])]  # Concatenate prompt + response \n",
    "    return tokenizer_llama_32(combined_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = ft_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\On3B3\\anaconda3\\envs\\prodenv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#define training arguments with huggin face trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"CreativeAssistant_AI\",\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,  # Simulates larger batch sizes\n",
    "    warmup_steps=100,\n",
    "    max_steps=1000,  # Number of steps (adjust for longer training)\n",
    "    learning_rate=2e-4,  # LoRA fine-tuning works well with higher LR\n",
    "    fp16=True,  # Use mixed precision\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",  # No validation for now\n",
    "    report_to=\"none\",\n",
    "    label_names =[\"labels\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\On3B3\\AppData\\Local\\Temp\\ipykernel_7444\\2020239918.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer_llama_32, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer_llama_32,\n",
    "    data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\On3B3\\anaconda3\\envs\\prodenv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 54:07, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.064100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.3488978078961372, metrics={'train_runtime': 3253.1277, 'train_samples_per_second': 9.837, 'train_steps_per_second': 0.307, 'total_flos': 3.5936872169472e+16, 'train_loss': 0.3488978078961372, 'epoch': 1000.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"creativeAssistant_AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = trainer.evaluate()\n",
    "#print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell the story about Marcus Rasford debut Premier league goal? Marcus Rasford was born on 10th May 1994 in the city of London. He is a professional footballer who plays as a striker for the Premier League club Leicester City. He made his debut for the club in the 2014/15 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/16 season. He scored his first goal for the club in the 2015/\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model='creativeAssistant_AI',temperature=0.7, top_p=0.4, top_k=30)\n",
    "prompt = \"Tell the story about Marcus Rasford debut Premier league goal?\"\n",
    "output = pipe(prompt, truncation= True, max_length=200)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Tell the story about Marcus Rasford debut Premier league goal? How many goals did he score in his first 14 games in the premier league? How many goals did he score in his second 14 games in the premier league? How many goals did he score in his third 14 games in the premier league? How many goals did he score in his fourth 14 games in the premier league? How many goals did he score in his fifth 14 games in the premier league? How many goals did he score in his sixth 14 games in the premier league? How many goals did he score in his seventh 14 games in the premier league? How many goals did he score in his eighth 14 games in the premier league? How many goals did he score in his ninth 14 games in the premier league? How many goals did he score in his tenth 14 games in the premier league? How many goals did he score in his eleventh 14 games in the premier league'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text to Embeddings \n",
    "\n",
    "using bert based transformer: SBERT\n",
    "does not require manual tokenization of text input\n",
    "model handles tokenization internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#print(\"Embedding model loaded!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer and model from Huggin Face\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    ''' Generates sentence embeddings using transformer all-MiniLM-L6-v2 '''\n",
    "    inputs = tokenizer(text,\n",
    "                       return_tensors ='pt',\n",
    "                       padding= True,\n",
    "                       truncation= True\n",
    "                       )\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    \n",
    "    #Mean pooling - Take the average of the last hidden state\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    #text = item.get(\"content\",\"\") #extract content \n",
    "    item[\"embedding\"] = get_embedding(item[\"cleaned_content\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embeddings to json file\n",
    "#with open(\"text_embeddings.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "#    json.dump(data,file, indent =4, ensure_ascii= False)\n",
    "\n",
    "#print(\"Embeddings saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview\n",
    "slices = data[1:2]\n",
    "\n",
    "for val in slices:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in FAISS\n",
    "\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if faiss installed\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"FAISS is installed!\")\n",
    "except ImportError:\n",
    "    print(\"FAISS is NOT installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installed faiss-gpu in conda terminal Window ; \n",
    "\n",
    "conda install conda-forge::faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Faiss index\n",
    "\n",
    "#import faiss\n",
    "#import numpy as np\n",
    "\n",
    "#dimension based on embedding size\n",
    "embedding_dim = 384 #this is based on expected output size of SBERT model\n",
    "\n",
    "#create Faiss index with GPU\n",
    "res = faiss.StandardGpuResources() \n",
    "index = faiss.GpuIndexFlatL2(res, embedding_dim) #L2 distance for similarity\n",
    "\n",
    "#convert embeddings to Faiss format \n",
    "embedding_matrix =np.array([item[\"embedding\"] for item in data], dtype =np.float32) #convert to float 32 for Faiss\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "\n",
    "# Verify index size\n",
    "print(f\"FAISS index contains {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "#faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "#print(\"FAISS index saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings into a numpy array\n",
    "embedding_matrix = np.array([item[\"embedding\"] for item in data], dtype=np.float32)\n",
    "\n",
    "# Create FAISS index (for efficient similarity search)\n",
    "dimension = embedding_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
