{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install bitsandbytes\n",
    "#!pip install accelerate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Test LLM (llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign llama 3.3 to variable name\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "#\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Load Model on CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#model check points for memory optimization\n",
    "#quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                       device_map=\"auto\",\n",
    "                                                       low_cpu_mem_usage=True,\n",
    "                                                       torch_dtype=torch.bfloat16,\n",
    "                                                       quantization_config=quantization_config)  #.to(device)\n",
    "\n",
    "#print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model.hf_device_map)  # Shows layer-device mapping\n",
    "#if model output {'': 0} means the entire model is loaded on GPU 0 (CUDA device 0)#\n",
    "#This confirms the model is running on CUDA and has been correctly assigned by device_map=\"auto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#check that model is running on GPU\n",
    "\n",
    "# Check if gpu is available\n",
    "print(torch.cuda.is_available())  \n",
    "\n",
    "# Return the number of GPUs available\n",
    "print(torch.cuda.device_count())  \n",
    "\n",
    "# Shows the GPU model\n",
    "print(torch.cuda.get_device_name(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for your LLaMA model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#input_text = \"What is your opinion on Manchester United?\"\n",
    "#inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    output = quantized_model.generate(**inputs,\n",
    "#                                      max_length=150,\n",
    "#                                      temperature=0.7,\n",
    "#                                      top_k = 54,\n",
    "#                                      top_p=0.9,\n",
    "#                                      repetition_penalty = 1.3,\n",
    "#                                      eos_token_id=tokenizer.eos_token_id\n",
    "#                                      )\n",
    "\n",
    "#response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts\n",
    "input_texts = [\"How does Manchester United’s dominance in the 1990s compare to Manchester City’s recent success?\",\n",
    "               \"What is your opinion on Manchester United?\",\n",
    "               \"Who is the most underrated midfielder in the Premier League right now, and why?\",\n",
    "               \"How has Arsenal’s playing style changed under Mikel Arteta compared to previous managers?\",\n",
    "]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_response(user_input, quantized_model,model_id):\n",
    "    \"\"\"\n",
    "    Generates a response from the quantized model for a single input or a list of inputs.\n",
    "\n",
    "    Args:\n",
    "        user_input (str or list): The input text(s) for the model.\n",
    "        quantized_model: The quantized model instance.\n",
    "        model_id (str): The model ID for loading the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of input-response pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Set padding side to avoid incorrect input handling\n",
    "    tokenizer.padding_side = \"left\"  \n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure there's a pad token set\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Ensure user_input is a list\n",
    "    if not isinstance(user_input, list):\n",
    "        user_input = [user_input]  # Convert single string to a list\n",
    "\n",
    "    # Process each sentence in the user input\n",
    "    for sent in user_input:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = quantized_model.generate(\n",
    "               **inputs,\n",
    "                max_length=250,\n",
    "                temperature=0.6,\n",
    "                top_k=27,\n",
    "                top_p=0.6,\n",
    "                repetition_penalty=1.9, #higher values discourage text generation with tokens in prompt\n",
    "                eos_token_id= tokenizer.eos_token_id,\n",
    "                do_sample = True\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Avoid redundancy: ensure the response does not repeat the input\n",
    "        if response.startswith(sent):\n",
    "            response = response[len(sent):].strip()\n",
    "\n",
    "        results.append((sent, response))  # Store input-response pair\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('How does Manchester United’s dominance in the 1990s compare to Manchester City’s recent success?',\n",
       "  'The answer is simple: it doesn’t. While there are many factors that have contributed towards Man Utd being a dominant force, one of them has been their ability and willingness (and sometimes lack thereof) when recruiting.\\nThe following article will examine some key players who were recruited by both clubs during different periods – specifically looking at those from before they became world beaters; after becoming World Beatners for good! So let us begin!\\nManchester united was founded on November\\n1st,1919 as an amalgamation between several smaller teams including Everton F.C., Bolton Wanderer FC &apos ; s Rangers Football Club plus Preston North End A.F C. However despite this initial merger all four sides remained independent until January whereupon each club had its own identity once again with only two exceptions i.e Liverpool football team which joined up later than others while Arsenal did so earlier still but without any other changes happening afterwards till April.\\nIn terms what made these new entities stand out amongst previous ones then we can say just about everything else except maybe money because even though most people thought differently initially regarding finances nothing would ever come close compared previously especially since every single player'),\n",
       " ('What is your opinion on Manchester United?',\n",
       "  'Do you think they are the best team in England?\\nManchester City have been my favourite club since I was a child. They were one of three clubs that won their respective leagues (Premier League, FA Cup and UEFA Champions league) when we had only two teams.\\nI love watching them play because there’s always something to learn from every game; it never gets boring or repetitive like other big sides do! The players can’t be compared with anyone else as everyone has different strengths but what makes this side stand out above all others for me though – especially if any questions arise regarding who should start up next week then please feel free get hold onto whatever advice might help us win our 20th title together!\\nDo not worry about how many goals will come into question during each match by yourself alone just focus more closely at those things which need improvement rather than anything specific concerning themselves individually so far away too much emphasis placed upon individual player performance while ignoring overall group success rate means little unless someone decides whether he/she wants his/her own name associated directly back down below where everything started off before getting involved right now instead try focusing primarily around some sort thing involving either particular person(s),team etc..'),\n",
       " ('Who is the most underrated midfielder in the Premier League right now, and why?',\n",
       "  'Who are some of his best teammates at Manchester United?\\nI think he’s probably one or two steps behind him. I’d say it was a couple years ago but if you look back to last season when we played Liverpool then they were definitely better than us.\\nHe has been very good since coming on board for Man Utd – especially against Chelsea this year (2 goals). He had 3 assists vs Arsenal & got injured so can’t really be compared with anyone else there!\\nHis pace offensively makes up more that just being fast as well!'),\n",
       " ('How has Arsenal’s playing style changed under Mikel Arteta compared to previous managers?',\n",
       "  'Arsenal have been in the news recently with a new manager coming into place.\\xa0 The club is currently managed by former player and coach,\\xa0Mickart Ateara who took over from Unai Emiato after his side were relegated last season.\\nThe Gunners are one of many clubs that will be changing their coaches this summer as they try out different styles on how teams should play football but what do we know about Arsene Wenger?\\nWhat does it mean when you say ‘the players’ or “players”?’\\nIt means exactly like your name says – meaning something (or someone) else than itself; an idea which exists outside oneself: hence its use among philosophers for expressing ideas not present within them themselves. It can also refer specifically only if necessary so far at least because there may well exist other meanings depending upon context.\\nIn fact, I think most people would agree more strongly now regarding these two words being used interchangeably rather then trying desperately hard just once again hoping against hope somehow find yourself some kind soul willing enough time invested making sure nothing gets left behind leaving no stone unturned whatever might happen next week etc..etc…so please')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate model response\n",
    "agent_response(input_texts,quantized_model,model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model as expected generates responses. However, it tends to end its response mid sentence.\n",
    "Further research to determine potential cause and solutions for iterative experimentation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
