{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install bitsandbytes\n",
    "#!pip install accelerate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Test LLM (llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign llama 3.3 to variable name\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "#\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Load Model on CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#model check points for memory optimization\n",
    "#quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                       device_map=\"auto\",\n",
    "                                                       low_cpu_mem_usage=True,\n",
    "                                                       torch_dtype=torch.bfloat16,\n",
    "                                                       quantization_config=quantization_config)  #.to(device)\n",
    "\n",
    "#print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model.hf_device_map)  # Shows layer-device mapping\n",
    "#if model output {'': 0} means the entire model is loaded on GPU 0 (CUDA device 0)#\n",
    "#This confirms the model is running on CUDA and has been correctly assigned by device_map=\"auto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#check that model is running on GPU\n",
    "\n",
    "# Check if gpu is available\n",
    "print(torch.cuda.is_available())  \n",
    "\n",
    "# Return the number of GPUs available\n",
    "print(torch.cuda.device_count())  \n",
    "\n",
    "# Shows the GPU model\n",
    "print(torch.cuda.get_device_name(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for your LLaMA model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#input_text = \"What is your opinion on Manchester United?\"\n",
    "#inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    output = quantized_model.generate(**inputs,\n",
    "#                                      max_length=150,\n",
    "#                                      temperature=0.7,\n",
    "#                                      top_k = 54,\n",
    "#                                      top_p=0.9,\n",
    "#                                      repetition_penalty = 1.3,\n",
    "#                                      eos_token_id=tokenizer.eos_token_id\n",
    "#                                      )\n",
    "\n",
    "#response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts\n",
    "input_texts = [\"How does Manchester United’s dominance in the 1990s compare to Manchester City’s recent success?\",\n",
    "               \"What is your opinion on Manchester United?\",\n",
    "               \"Who is the most underrated midfielder in the Premier League right now, and why?\",\n",
    "               \"How has Arsenal’s playing style changed under Mikel Arteta compared to previous managers?\",\n",
    "]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_response(user_input, quantized_model,model_id):\n",
    "    \"\"\"\n",
    "    Generates a response from the quantized model for a single input or a list of inputs.\n",
    "\n",
    "    Args:\n",
    "        user_input (str or list): The input text(s) for the model.\n",
    "        quantized_model: The quantized model instance.\n",
    "        model_id (str): The model ID for loading the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of input-response pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Set padding side to avoid incorrect input handling\n",
    "    tokenizer.padding_side = \"left\"  \n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure there's a pad token set\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Ensure user_input is a list\n",
    "    if not isinstance(user_input, list):\n",
    "        user_input = [user_input]  # Convert single string to a list\n",
    "\n",
    "    # Process each sentence in the user input\n",
    "    for sent in user_input:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = quantized_model.generate(\n",
    "               **inputs,\n",
    "                max_length=250,\n",
    "                temperature=0.6,\n",
    "                top_k=27,\n",
    "                top_p=0.6,\n",
    "                repetition_penalty=1.9, #higher values discourage text generation with tokens in prompt\n",
    "                eos_token_id= tokenizer.eos_token_id,\n",
    "                do_sample = True\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Avoid redundancy: ensure the response does not repeat the input\n",
    "        if response.startswith(sent):\n",
    "            response = response[len(sent):].strip()\n",
    "\n",
    "        results.append((sent, response))  # Store input-response pair\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('How does Manchester United’s dominance in the 1990s compare to Manchester City’s recent success?',\n",
       "  'What role did players like Beckham, Ronaldo and Zidane play?\\nManchester is a city with many different personalities. It has been called “the most English of cities” because it was settled by British settlers who were looking for land on which they could settle their own people.\\nIt also had its fair share when England won Euro ’96 after beating France at Wembley Stadium (which incidentally hosted World Cup games before). The victory gave us our first major championship since we last took one back home – even though that title went unclaimed until Brazil beat Germany two years later!\\nThe next time you’re wondering what happened between David Harvey & Co.’david harvey’ s reign as manager or coach; just think about how long ago those days seemed!'),\n",
       " ('What is your opinion on Manchester United?',\n",
       "  \"Do you think they are the greatest club in history?\\nI would have to say yes, I am a huge fan of Man Utd and always will be. They were my first team when i was younger so it's hard for me not being biased but that said if we look at their current squad then there has been no better than them.\\nThe only thing about man utd which makes us different from other clubs like Liverpool or Chelsea etc who all seem equally as good (or worse)is our ability/willingness/whatever word u choose...to win trophies!\"),\n",
       " ('Who is the most underrated midfielder in the Premier League right now, and why?',\n",
       "  'We ask our experts for their thoughts.\\nThere are many players who have been underrated by pundits or fans alike. Here we look at some of those names to see if they deserve more recognition than others when it comes down this season’s transfer window:\\nThe player with arguably one\\nof best records as a footballer has had his name called out time after again during every single major award ceremony since 1998 – but what about him?\\nIf you were asked which was your favourite club from any era (from pre-World War II up until today), then there would be no doubt whatsoever that Manchester United will always take precedence over all other teams across England; indeed even beyond its own country borders too!\\nSo let us examine just how good he really should’ve played against Liverpool last weekend…\\nIn terms on overall goals scored per game: Chelsea won’t win anything yet! They’re not going anywhere soon though so here goes my prediction regarding them winning another league title before long…..\\nLet me tell everyone else I know first off because nobody knows better where these things come together between two clubs like myself does anyway!!\\nI think Arsenal can do much worse considering only three'),\n",
       " ('How has Arsenal’s playing style changed under Mikel Arteta compared to previous managers?',\n",
       "  '(Photo: Getty Images)\\nArsenal have won the Premier League title three times in their history, but what are they known for?\\nThe Gunners were one of only a few clubs that could boast winning every major league competition. They had already achieved this feat by 2013 when Arsene Wenger became manager.\\nWenger was brought into management at age-40 and he is now approaching his own retirement from football after an impressive career as player with Paris Saint-Germain FC\\nBut how much does it matter who you play against if your team wins or loses – especially during matches where there isn’t any action going on between teams like we see today due too Covid restrictions?\\nIt can be difficult deciding which club will win because each side plays differently than others so let us take some time out together discussing all things related specifically regarding our favorite topic!\\nWhat Is The Difference Between A Football Manager And An Academy Coach In England Today Compared To Previous Managers At Other Clubs During Their Time As Players Or Coaches For Your Team That You Are Playing Against Right Now Who Has Won More Titles Than Any Of Them So Far While Also Discussing What Makes These Two Teams Different From')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate model response\n",
    "agent_response(input_texts,quantized_model,model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model as expected generates responses. However, it tends to end its response mid sentence.\n",
    "Further research to determine potential cause and solutions for iterative experimentation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
